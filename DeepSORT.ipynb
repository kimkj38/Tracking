{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn_matching.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 유클리디안 거리\n",
    "def _pdist(a, b):\n",
    "    \"\"\"Compute pair-wise squared distance between points in `a` and `b`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : array_like\n",
    "        An NxM matrix of N samples of dimensionality M.\n",
    "    b : array_like\n",
    "        An LxM matrix of L samples of dimensionality M.\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Returns a matrix of size len(a), len(b) such that eleement (i, j)\n",
    "        contains the squared distance between `a[i]` and `b[j]`.\n",
    "    \"\"\"\n",
    "    a, b = np.asarray(a), np.asarray(b)\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return np.zeros((len(a), len(b)))\n",
    "    a2, b2 = np.square(a).sum(axis=1), np.square(b).sum(axis=1)\n",
    "    r2 = -2. * np.dot(a, b.T) + a2[:, None] + b2[None, :]\n",
    "    r2 = np.clip(r2, 0., float(np.inf))\n",
    "    return r2\n",
    "\n",
    "# 코사인 유사도\n",
    "def _cosine_distance(a, b, data_is_normalized=False):\n",
    "    \"\"\"Compute pair-wise cosine distance between points in `a` and `b`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : array_like\n",
    "        An NxM matrix of N samples of dimensionality M.\n",
    "    b : array_like\n",
    "        An LxM matrix of L samples of dimensionality M.\n",
    "    data_is_normalized : Optional[bool]\n",
    "        If True, assumes rows in a and b are unit length vectors.\n",
    "        Otherwise, a and b are explicitly normalized to lenght 1.\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Returns a matrix of size len(a), len(b) such that eleement (i, j)\n",
    "        contains the squared distance between `a[i]` and `b[j]`.\n",
    "    \"\"\"\n",
    "    if not data_is_normalized:\n",
    "        a = np.asarray(a) / np.linalg.norm(a, axis=1, keepdims=True)\n",
    "        b = np.asarray(b) / np.linalg.norm(b, axis=1, keepdims=True)\n",
    "    return 1. - np.dot(a, b.T)\n",
    "\n",
    "# 최소 유클리디안\n",
    "def _nn_euclidean_distance(x, y):\n",
    "    \"\"\" Helper function for nearest neighbor distance metric (Euclidean).\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        A matrix of N row-vectors (sample points).\n",
    "    y : ndarray\n",
    "        A matrix of M row-vectors (query points).\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        A vector of length M that contains for each entry in `y` the\n",
    "        smallest Euclidean distance to a sample in `x`.\n",
    "    \"\"\"\n",
    "    distances = _pdist(x, y)\n",
    "    return np.maximum(0.0, distances.min(axis=0))\n",
    "\n",
    "# 최소 코사인\n",
    "def _nn_cosine_distance(x, y):\n",
    "    \"\"\" Helper function for nearest neighbor distance metric (cosine).\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        A matrix of N row-vectors (sample points).\n",
    "    y : ndarray\n",
    "        A matrix of M row-vectors (query points).\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        A vector of length M that contains for each entry in `y` the\n",
    "        smallest cosine distance to a sample in `x`.\n",
    "    \"\"\"\n",
    "    distances = _cosine_distance(x, y)\n",
    "    return distances.min(axis=0)\n",
    "\n",
    "# cost_matrix\n",
    "class NearestNeighborDistanceMetric(object):\n",
    "    \"\"\"\n",
    "    A nearest neighbor distance metric that, for each target, returns\n",
    "    the closest distance to any sample that has been observed so far.\n",
    "    Parameters\n",
    "    ----------\n",
    "    metric : str\n",
    "        Either \"euclidean\" or \"cosine\".\n",
    "    matching_threshold: float\n",
    "        The matching threshold. Samples with larger distance are considered an\n",
    "        invalid match.\n",
    "    budget : Optional[int]\n",
    "        If not None, fix samples per class to at most this number. Removes\n",
    "        the oldest samples when the budget is reached.\n",
    "    Attributes\n",
    "    ----------\n",
    "    samples : Dict[int -> List[ndarray]]\n",
    "        A dictionary that maps from target identities to the list of samples\n",
    "        that have been observed so far.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metric, matching_threshold, budget=None):\n",
    "\n",
    "\n",
    "        if metric == \"euclidean\":\n",
    "            self._metric = _nn_euclidean_distance\n",
    "        elif metric == \"cosine\":\n",
    "            self._metric = _nn_cosine_distance\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid metric; must be either 'euclidean' or 'cosine'\")\n",
    "        self.matching_threshold = matching_threshold\n",
    "        self.budget = budget # 클래스 당 샘플 수를 고정. 오래된 샘플부터 지운다. \n",
    "        self.samples = {} \n",
    "\n",
    "    def partial_fit(self, features, targets, active_targets):\n",
    "        \"\"\"Update the distance metric with new data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : ndarray\n",
    "            An NxM matrix of N features of dimensionality M.\n",
    "        targets : ndarray\n",
    "            An integer array of associated target identities.\n",
    "        active_targets : List[int]\n",
    "            A list of targets that are currently present in the scene.\n",
    "        \"\"\"\n",
    "        \n",
    "        # setdefault(a,b)\n",
    "        # 딕셔너리에 a라는 key가 있으면 그에 해당하는 value 리턴, a가 없으면 default값 b 리턴        \n",
    "        for feature, target in zip(features, targets):\n",
    "            self.samples.setdefault(target, []).append(feature) # samples = {target(track_id): feature}\n",
    "            if self.budget is not None:\n",
    "                self.samples[target] = self.samples[target][-self.budget:] # 최근에 관측된 것들 budget수만큼 남긴다\n",
    "        # confirmed track만 남긴다\n",
    "        self.samples = {k: self.samples[k] for k in active_targets}\n",
    "    \n",
    "    # cost_matrix 생성\n",
    "    def distance(self, features, targets):\n",
    "        \"\"\"Compute distance between features and targets.\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : ndarray\n",
    "            An NxM matrix of N features of dimensionality M.\n",
    "        targets : List[int]\n",
    "            A list of targets to match the given `features` against.\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Returns a cost matrix of shape len(targets), len(features), where\n",
    "            element (i, j) contains the closest squared distance between\n",
    "            `targets[i]` and `features[j]`.\n",
    "        \"\"\"\n",
    "        cost_matrix = np.zeros((len(targets), len(features))) # cost_matrix 초기화(행이 tracks, 열이 detections)\n",
    "        for i, target in enumerate(targets):\n",
    "            cost_matrix[i, :] = self._metric(self.samples[target], features)\n",
    "        return cost_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear_assignment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import numpy as np\n",
    "# from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "from . import kalman_filter\n",
    "\n",
    "\n",
    "INFTY_COST = 1e+5\n",
    "\n",
    "# unmatched detections, unmatched_tracks, matches 구분\n",
    "def min_cost_matching(\n",
    "        distance_metric, max_distance, tracks, detections, track_indices=None,\n",
    "        detection_indices=None):\n",
    "    \"\"\"Solve linear assignment problem.\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\n",
    "        The distance metric is given a list of tracks and detections as well as\n",
    "        a list of N track indices and M detection indices. The metric should\n",
    "        return the NxM dimensional cost matrix, where element (i, j) is the\n",
    "        association cost between the i-th track in the given track indices and\n",
    "        the j-th detection in the given detection_indices.\n",
    "    max_distance : float\n",
    "        Gating threshold. Associations with cost larger than this value are\n",
    "        disregarded.\n",
    "    tracks : List[track.Track]\n",
    "        A list of predicted tracks at the current time step.\n",
    "    detections : List[detection.Detection]\n",
    "        A list of detections at the current time step.\n",
    "    track_indices : List[int]\n",
    "        List of track indices that maps rows in `cost_matrix` to tracks in\n",
    "        `tracks` (see description above).\n",
    "    detection_indices : List[int]\n",
    "        List of detection indices that maps columns in `cost_matrix` to\n",
    "        detections in `detections` (see description above).\n",
    "    Returns\n",
    "    -------\n",
    "    (List[(int, int)], List[int], List[int])\n",
    "        Returns a tuple with the following three entries:\n",
    "        * A list of matched track and detection indices.\n",
    "        * A list of unmatched track indices.\n",
    "        * A list of unmatched detection indices.\n",
    "    \"\"\"\n",
    "    # index 없으면 생성\n",
    "    if track_indices is None:\n",
    "        track_indices = np.arange(len(tracks))\n",
    "    if detection_indices is None:\n",
    "        detection_indices = np.arange(len(detections))\n",
    "\n",
    "    if len(detection_indices) == 0 or len(track_indices) == 0:\n",
    "        return [], track_indices, detection_indices  # Nothing to match.\n",
    "\n",
    "    # cost_matrix 생성\n",
    "    cost_matrix = distance_metric(\n",
    "        tracks, detections, track_indices, detection_indices)\n",
    "    # max_distance보다 큰 cost는 max_distance로 수정\n",
    "    cost_matrix[cost_matrix > max_distance] = max_distance + 1e-5\n",
    "    # 행,열의 인덱스\n",
    "    row_indices, col_indices = linear_assignment(cost_matrix)\n",
    "\n",
    "    matches, unmatched_tracks, unmatched_detections = [], [], []\n",
    "    \n",
    "    # unmatched detections\n",
    "    for col, detection_idx in enumerate(detection_indices):\n",
    "        if col not in col_indices:\n",
    "            unmatched_detections.append(detection_idx)\n",
    "    \n",
    "    # unmatched_tracks\n",
    "    for row, track_idx in enumerate(track_indices):\n",
    "        if row not in row_indices:\n",
    "            unmatched_tracks.append(track_idx)\n",
    "    \n",
    "    # max_distance보다 큰 cost에 해당하는 track과 detection들은 unmatch 리스트로\n",
    "    # 나머지는 matches 리스트로 (track_idx, detection_idx)\n",
    "    for row, col in zip(row_indices, col_indices):\n",
    "        track_idx = track_indices[row]\n",
    "        detection_idx = detection_indices[col]\n",
    "        if cost_matrix[row, col] > max_distance:\n",
    "            unmatched_tracks.append(track_idx)\n",
    "            unmatched_detections.append(detection_idx)\n",
    "        else:\n",
    "            matches.append((track_idx, detection_idx))\n",
    "    return matches, unmatched_tracks, unmatched_detections\n",
    "\n",
    "\n",
    "def matching_cascade(\n",
    "        distance_metric, max_distance, cascade_depth, tracks, detections,\n",
    "        track_indices=None, detection_indices=None):\n",
    "    \"\"\"Run matching cascade.\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_metric : Callable[List[Track], List[Detection], List[int], List[int]) -> ndarray\n",
    "        The distance metric is given a list of tracks and detections as well as\n",
    "        a list of N track indices and M detection indices. The metric should\n",
    "        return the NxM dimensional cost matrix, where element (i, j) is the\n",
    "        association cost between the i-th track in the given track indices and\n",
    "        the j-th detection in the given detection indices.\n",
    "    max_distance : float\n",
    "        Gating threshold. Associations with cost larger than this value are\n",
    "        disregarded.\n",
    "    cascade_depth: int\n",
    "        The cascade depth, should be se to the maximum track age.\n",
    "    tracks : List[track.Track]\n",
    "        A list of predicted tracks at the current time step.\n",
    "    detections : List[detection.Detection]\n",
    "        A list of detections at the current time step.\n",
    "    track_indices : Optional[List[int]]\n",
    "        List of track indices that maps rows in `cost_matrix` to tracks in\n",
    "        `tracks` (see description above). Defaults to all tracks.\n",
    "    detection_indices : Optional[List[int]]\n",
    "        List of detection indices that maps columns in `cost_matrix` to\n",
    "        detections in `detections` (see description above). Defaults to all\n",
    "        detections.\n",
    "    Returns\n",
    "    -------\n",
    "    (List[(int, int)], List[int], List[int])\n",
    "        Returns a tuple with the following three entries:\n",
    "        * A list of matched track and detection indices.\n",
    "        * A list of unmatched track indices.\n",
    "        * A list of unmatched detection indices.\n",
    "    \"\"\"\n",
    "    if track_indices is None:\n",
    "        track_indices = list(range(len(tracks)))\n",
    "    if detection_indices is None:\n",
    "        detection_indices = list(range(len(detections)))\n",
    "\n",
    "    # unmatched_detections를 초기화한다. 매칭 시 빼준다.\n",
    "    unmatched_detections = detection_indices\n",
    "    matches = []\n",
    "    \n",
    "    # age에 대해 반복문\n",
    "    for level in range(cascade_depth):\n",
    "        if len(unmatched_detections) == 0:  # No detections left\n",
    "            break\n",
    "\n",
    "        track_indices_l = [\n",
    "            k for k in track_indices\n",
    "            if tracks[k].time_since_update == 1 + level\n",
    "        ]\n",
    "        if len(track_indices_l) == 0:  # Nothing to match at this level\n",
    "            continue\n",
    "\n",
    "        matches_l, _, unmatched_detections = \\\n",
    "            min_cost_matching(\n",
    "                distance_metric, max_distance, tracks, detections,\n",
    "                track_indices_l, unmatched_detections)\n",
    "        matches += matches_l\n",
    "    unmatched_tracks = list(set(track_indices) - set(k for k, _ in matches))\n",
    "    return matches, unmatched_tracks, unmatched_detections\n",
    "\n",
    "# 카이제곱분포의 95% 신뢰구간만 고려하여 확률이 떨어지는 association 배제\n",
    "def gate_cost_matrix(\n",
    "        kf, cost_matrix, tracks, detections, track_indices, detection_indices,\n",
    "        gated_cost=INFTY_COST, only_position=False):\n",
    "    \"\"\"Invalidate infeasible entries in cost matrix based on the state\n",
    "    distributions obtained by Kalman filtering.\n",
    "    Parameters\n",
    "    ----------\n",
    "    kf : The Kalman filter.\n",
    "    cost_matrix : ndarray\n",
    "        The NxM dimensional cost matrix, where N is the number of track indices\n",
    "        and M is the number of detection indices, such that entry (i, j) is the\n",
    "        association cost between `tracks[track_indices[i]]` and\n",
    "        `detections[detection_indices[j]]`.\n",
    "    tracks : List[track.Track]\n",
    "        A list of predicted tracks at the current time step.\n",
    "    detections : List[detection.Detection]\n",
    "        A list of detections at the current time step.\n",
    "    track_indices : List[int]\n",
    "        List of track indices that maps rows in `cost_matrix` to tracks in\n",
    "        `tracks` (see description above).\n",
    "    detection_indices : List[int]\n",
    "        List of detection indices that maps columns in `cost_matrix` to\n",
    "        detections in `detections` (see description above).\n",
    "    gated_cost : Optional[float]\n",
    "        Entries in the cost matrix corresponding to infeasible associations are\n",
    "        set this value. Defaults to a very large value.\n",
    "    only_position : Optional[bool]\n",
    "        If True, only the x, y position of the state distribution is considered\n",
    "        during gating. Defaults to False.\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Returns the modified cost matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    자유도에 따른 값\n",
    "    chi2inv95 = {\n",
    "    1: 3.8415,\n",
    "    2: 5.9915,\n",
    "    3: 7.8147,\n",
    "    4: 9.4877,\n",
    "    5: 11.070,\n",
    "    6: 12.592,\n",
    "    7: 14.067,\n",
    "    8: 15.507,\n",
    "    9: 16.919}\n",
    "    \n",
    "    \"\"\"\n",
    "    gating_dim = 2 if only_position else 4 # 카이제곱분포의 95% 신뢰구간\n",
    "    gating_threshold = kalman_filter.chi2inv95[gating_dim]\n",
    "    \n",
    "    # detections의 xyah좌표 리스트\n",
    "    measurements = np.asarray(\n",
    "        [detections[i].to_xyah() for i in detection_indices])\n",
    "    \n",
    "    for row, track_idx in enumerate(track_indices):\n",
    "        track = tracks[track_idx]\n",
    "        # Compute gating distance between state distribution and measurements\n",
    "        gating_distance = kf.gating_distance(\n",
    "            track.mean, track.covariance, measurements, only_position)\n",
    "        \n",
    "        # 임계값보다 높으면 1e+5=7.718 높은 숫자??배제하는 방법?\n",
    "        cost_matrix[row, gating_distance > gating_threshold] = gated_cost \n",
    "    return cost_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# track.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackState:\n",
    "    \"\"\"\n",
    "    Enumeration type for the single target track state. Newly created tracks are\n",
    "    classified as `tentative` until enough evidence has been collected. Then,\n",
    "    the track state is changed to `confirmed`. Tracks that are no longer alive\n",
    "    are classified as `deleted` to mark them for removal from the set of active\n",
    "    tracks.\n",
    "    \"\"\"\n",
    "    # 확실히 새로운 객체라고 인식하기 전까지는 Tentative, 확실한 객체에 대해서는 Confirmed, 사라진 객체는 Deleted로 분류\n",
    "    Tentative = 1\n",
    "    Confirmed = 2\n",
    "    Deleted = 3\n",
    "\n",
    "\n",
    "class Track:\n",
    "    \"\"\"\n",
    "    A single target track with state space `(x, y, a, h)` and associated\n",
    "    velocities, where `(x, y)` is the center of the bounding box, `a` is the\n",
    "    aspect ratio and `h` is the height.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : ndarray\n",
    "        Mean vector of the initial state distribution.\n",
    "    covariance : ndarray\n",
    "        Covariance matrix of the initial state distribution.\n",
    "    track_id : int\n",
    "        A unique track identifier.\n",
    "    n_init : int\n",
    "        Number of consecutive detections before the track is confirmed. The\n",
    "        track state is set to `Deleted` if a miss occurs within the first\n",
    "        `n_init` frames.\n",
    "    max_age : int\n",
    "        The maximum number of consecutive misses before the track state is\n",
    "        set to `Deleted`.\n",
    "    feature : Optional[ndarray]\n",
    "        Feature vector of the detection this track originates from. If not None,\n",
    "        this feature is added to the `features` cache.\n",
    "    Attributes\n",
    "    ----------\n",
    "    mean : ndarray\n",
    "        Mean vector of the initial state distribution.\n",
    "    covariance : ndarray\n",
    "        Covariance matrix of the initial state distribution.\n",
    "    track_id : int\n",
    "        A unique track identifier.\n",
    "    hits : int\n",
    "        Total number of measurement updates.\n",
    "    age : int\n",
    "        Total number of frames since first occurance.\n",
    "    time_since_update : int\n",
    "        Total number of frames since last measurement update.\n",
    "    state : TrackState\n",
    "        The current track state.\n",
    "    features : List[ndarray]\n",
    "        A cache of features. On each measurement update, the associated feature\n",
    "        vector is added to this list.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, covariance, track_id, n_init, max_age,\n",
    "                 feature=None):\n",
    "        self.mean = mean\n",
    "        self.covariance = covariance\n",
    "        self.track_id = track_id\n",
    "        self.hits = 1 # measurement update 횟수\n",
    "        self.age = 1  # 객체가 나타난 이후 frame의 수\n",
    "        self.time_since_update = 0 # 마지막 measurement update 이후 frame의 수\n",
    "\n",
    "        self.state = TrackState.Tentative\n",
    "        self.features = []\n",
    "        if feature is not None:\n",
    "            self.features.append(feature)\n",
    "\n",
    "        self._n_init = n_init   # Confirmed 상태가 되기까지 필요한 연속적인 매칭 frame의 수\n",
    "        self._max_age = max_age # age>max_age가 되면(매칭이 오랫동안 안되면) Delete\n",
    "\n",
    "    def to_tlwh(self):\n",
    "        \"\"\"Get current position in bounding box format `(top left x, top left y,\n",
    "        width, height)`.\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The bounding box.\n",
    "        \"\"\"\n",
    "        ret = self.mean[:4].copy()\n",
    "        ret[2] *= ret[3]\n",
    "        ret[:2] -= ret[2:] / 2\n",
    "        return ret\n",
    "\n",
    "    def to_tlbr(self):\n",
    "        \"\"\"Get current position in bounding box format `(min x, miny, max x,\n",
    "        max y)`.\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The bounding box.\n",
    "        \"\"\"\n",
    "        ret = self.to_tlwh()\n",
    "        ret[2:] = ret[:2] + ret[2:]\n",
    "        return ret\n",
    "\n",
    "    def predict(self, kf):\n",
    "        \"\"\"Propagate the state distribution to the current time step using a\n",
    "        Kalman filter prediction step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        kf : kalman_filter.KalmanFilter\n",
    "            The Kalman filter.\n",
    "        \"\"\"\n",
    "        self.mean, self.covariance = kf.predict(self.mean, self.covariance)\n",
    "        self.age += 1\n",
    "        self.time_since_update += 1\n",
    "\n",
    "    def update(self, kf, detection):\n",
    "        \"\"\"Perform Kalman filter measurement update step and update the feature\n",
    "        cache.\n",
    "        Parameters\n",
    "        ----------\n",
    "        kf : kalman_filter.KalmanFilter\n",
    "            The Kalman filter.\n",
    "        detection : Detection\n",
    "            The associated detection.\n",
    "        \"\"\"\n",
    "        self.mean, self.covariance = kf.update(\n",
    "            self.mean, self.covariance, detection.to_xyah())\n",
    "        self.features.append(detection.feature)\n",
    "\n",
    "        self.hits += 1\n",
    "        self.time_since_update = 0 # 업데이트 시 초기화. 이 값이 계속 늘어나면 Delete 된다.\n",
    "        # Tentarive track이 n_init 이상의 hits를 가지면 Confrimed로 바뀐다\n",
    "        if self.state == TrackState.Tentative and self.hits >= self._n_init:\n",
    "            self.state = TrackState.Confirmed\n",
    "    \n",
    "    # Delete하는 경우들\n",
    "    def mark_missed(self):\n",
    "        \"\"\"Mark this track as missed (no association at the current time step).\n",
    "        \"\"\"\n",
    "        # Tentative track이 association 되지 않을 경우 Delete\n",
    "        if self.state == TrackState.Tentative:\n",
    "            self.state = TrackState.Deleted\n",
    "        \n",
    "        # max_age가 넘도록 assocation이 안될 경우 Delete\n",
    "        elif self.time_since_update > self._max_age:\n",
    "            self.state = TrackState.Deleted\n",
    "    \n",
    "    def is_tentative(self):\n",
    "        \"\"\"Returns True if this track is tentative (unconfirmed).\n",
    "        \"\"\"\n",
    "        return self.state == TrackState.Tentative\n",
    "\n",
    "    def is_confirmed(self):\n",
    "        \"\"\"Returns True if this track is confirmed.\"\"\"\n",
    "        return self.state == TrackState.Confirmed\n",
    "\n",
    "    def is_deleted(self):\n",
    "        \"\"\"Returns True if this track is dead and should be deleted.\"\"\"\n",
    "        return self.state == TrackState.Deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tracker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import numpy as np\n",
    "from . import kalman_filter\n",
    "from . import linear_assignment\n",
    "from . import iou_matching\n",
    "from .track import Track\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    \"\"\"\n",
    "    This is the multi-target tracker.\n",
    "    Parameters\n",
    "    ----------\n",
    "    metric : nn_matching.NearestNeighborDistanceMetric\n",
    "        A distance metric for measurement-to-track association.\n",
    "    max_age : int\n",
    "        Maximum number of missed misses before a track is deleted.\n",
    "    n_init : int\n",
    "        Number of consecutive detections before the track is confirmed. The\n",
    "        track state is set to `Deleted` if a miss occurs within the first\n",
    "        `n_init` frames.\n",
    "    Attributes\n",
    "    ----------\n",
    "    metric : nn_matching.NearestNeighborDistanceMetric\n",
    "        The distance metric used for measurement to track association.\n",
    "    max_age : int\n",
    "        Maximum number of missed misses before a track is deleted.\n",
    "    n_init : int\n",
    "        Number of frames that a track remains in initialization phase.\n",
    "    kf : kalman_filter.KalmanFilter\n",
    "        A Kalman filter to filter target trajectories in image space.\n",
    "    tracks : List[Track]\n",
    "        The list of active tracks at the current time step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metric, max_iou_distance=0.7, max_age=70, n_init=3):\n",
    "        self.metric = metric\n",
    "        self.max_iou_distance = max_iou_distance\n",
    "        self.max_age = max_age\n",
    "        self.n_init = n_init\n",
    "\n",
    "        self.kf = kalman_filter.KalmanFilter()\n",
    "        self.tracks = []\n",
    "        self._next_id = 1\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"Propagate track state distributions one time step forward.\n",
    "        This function should be called once every time step, before `update`.\n",
    "        \"\"\"\n",
    "        for track in self.tracks:\n",
    "            track.predict(self.kf)\n",
    "\n",
    "    def update(self, detections):\n",
    "        \"\"\"Perform measurement update and track management.\n",
    "        Parameters\n",
    "        ----------\n",
    "        detections : List[deep_sort.detection.Detection]\n",
    "            A list of detections at the current time step.\n",
    "        \"\"\"\n",
    "        # Run matching cascade.\n",
    "        matches, unmatched_tracks, unmatched_detections = \\\n",
    "            self._match(detections)\n",
    "\n",
    "        # 매칭된 track 업데이트\n",
    "        for track_idx, detection_idx in matches:\n",
    "            self.tracks[track_idx].update(\n",
    "                self.kf, detections[detection_idx])\n",
    "        \n",
    "        # 매칭 안된 track 삭제(Tentetive는 즉시, Confirmed는 time_since_update > max_age의 경우만)\n",
    "        for track_idx in unmatched_tracks:\n",
    "            self.tracks[track_idx].mark_missed()\n",
    "        \n",
    "        # 매칭 안된 detection은 새로운 track으로 인지\n",
    "        for detection_idx in unmatched_detections:\n",
    "            self._initiate_track(detections[detection_idx])\n",
    "        \n",
    "        # delete된 track들 리스트에서 제거\n",
    "        self.tracks = [t for t in self.tracks if not t.is_deleted()]\n",
    "\n",
    "        # Update distance metric.\n",
    "        # confirmed track에 대해서만 수행. 코사인 유사도.\n",
    "        active_targets = [t.track_id for t in self.tracks if t.is_confirmed()]\n",
    "        features, targets = [], []\n",
    "        for track in self.tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            features += track.features\n",
    "            targets += [track.track_id for _ in track.features]\n",
    "            track.features = []\n",
    "            \n",
    "        # samples={target(track_id): feature 리스트}, confirmed track에 대해서만 \n",
    "        self.metric.partial_fit(\n",
    "            np.asarray(features), np.asarray(targets), active_targets)\n",
    "\n",
    "    def _match(self, detections):\n",
    "\n",
    "        def gated_metric(tracks, dets, track_indices, detection_indices):\n",
    "            # features, targets 도출\n",
    "            features = np.array([dets[i].feature for i in detection_indices]) \n",
    "            targets = np.array([tracks[i].track_id for i in track_indices])   \n",
    "            \n",
    "            # cost_matrix 구하기\n",
    "            cost_matrix = self.metric.distance(features, targets)\n",
    "            \n",
    "            # 카이제곱분포 threshold\n",
    "            cost_matrix = linear_assignment.gate_cost_matrix(\n",
    "                self.kf, cost_matrix, tracks, dets, track_indices,\n",
    "                detection_indices)\n",
    "\n",
    "            return cost_matrix\n",
    "\n",
    "        # confirmed_tarcks와 unconfirmed_tracks 구분 \n",
    "        confirmed_tracks = [\n",
    "            i for i, t in enumerate(self.tracks) if t.is_confirmed()]\n",
    "        unconfirmed_tracks = [\n",
    "            i for i, t in enumerate(self.tracks) if not t.is_confirmed()]\n",
    "\n",
    "        # features 이용하여 confirmed track들 associate\n",
    "        matches_a, unmatched_tracks_a, unmatched_detections = \\\n",
    "            linear_assignment.matching_cascade(\n",
    "                gated_metric, self.metric.matching_threshold, self.max_age,\n",
    "                self.tracks, detections, confirmed_tracks)\n",
    "\n",
    "        # IOU 이용하여 unmatched and unconfirmed tracks와 unmatched detection associate\n",
    "        iou_track_candidates = unconfirmed_tracks + [\n",
    "            k for k in unmatched_tracks_a if\n",
    "            self.tracks[k].time_since_update == 1] \n",
    "        unmatched_tracks_a = [\n",
    "            k for k in unmatched_tracks_a if\n",
    "            self.tracks[k].time_since_update != 1]\n",
    "        matches_b, unmatched_tracks_b, unmatched_detections = \\\n",
    "            linear_assignment.min_cost_matching(\n",
    "                iou_matching.iou_cost, self.max_iou_distance, self.tracks,\n",
    "                detections, iou_track_candidates, unmatched_detections)\n",
    "\n",
    "        matches = matches_a + matches_b\n",
    "        unmatched_tracks = list(set(unmatched_tracks_a + unmatched_tracks_b))\n",
    "        return matches, unmatched_tracks, unmatched_detections\n",
    "\n",
    "    # 새로운 track\n",
    "    def _initiate_track(self, detection):\n",
    "        mean, covariance = self.kf.initiate(detection.to_xyah())\n",
    "        self.tracks.append(Track(\n",
    "            mean, covariance, self._next_id, self.n_init, self.max_age,\n",
    "            detection.feature))\n",
    "        self._next_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepsort.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from .deep.feature_extractor import Extractor\n",
    "from .sort.nn_matching import NearestNeighborDistanceMetric\n",
    "from .sort.preprocessing import non_max_suppression\n",
    "from .sort.detection import Detection\n",
    "from .sort.tracker import Tracker\n",
    "\n",
    "\n",
    "__all__ = ['DeepSort']\n",
    "\n",
    "\n",
    "class DeepSort(object):\n",
    "    def __init__(self, model_path, max_dist=0.2, min_confidence=0.3, nms_max_overlap=1.0, max_iou_distance=0.7, max_age=70, n_init=3, nn_budget=100, use_cuda=True):\n",
    "        self.min_confidence = min_confidence\n",
    "        self.nms_max_overlap = nms_max_overlap\n",
    "\n",
    "        self.extractor = Extractor(model_path, use_cuda=use_cuda)\n",
    "\n",
    "        max_cosine_distance = max_dist\n",
    "        nn_budget = 100\n",
    "        metric = NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "        self.tracker = Tracker(metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n",
    "\n",
    "    def update(self, bbox_xywh, confidences, ori_img):\n",
    "        self.height, self.width = ori_img.shape[:2]\n",
    "        # generate detections\n",
    "        features = self._get_features(bbox_xywh, ori_img) # bbox의 특징벡터 추출\n",
    "        bbox_tlwh = self._xywh_to_tlwh(bbox_xywh)\n",
    "        \n",
    "        # min_confidence 이상의 detection들만 뽑아낸다\n",
    "        detections = [Detection(bbox_tlwh[i], conf, features[i]) for i,conf in enumerate(confidences) if conf>self.min_confidence]\n",
    "\n",
    "        # run on non-maximum supression(IOU가 nms_max_overlap 이상인 것들 삭제)\n",
    "        boxes = np.array([d.tlwh for d in detections])\n",
    "        scores = np.array([d.confidence for d in detections])\n",
    "        indices = non_max_suppression(boxes, self.nms_max_overlap, scores)\n",
    "        detections = [detections[i] for i in indices]\n",
    "\n",
    "        # update tracker\n",
    "        self.tracker.predict()\n",
    "        self.tracker.update(detections)\n",
    "\n",
    "        # output bbox identities\n",
    "        # 계속 매칭이 되었던 confirmed track들 ouput에 넣어준다\n",
    "        outputs = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            box = track.to_tlwh()\n",
    "            x1,y1,x2,y2 = self._tlwh_to_xyxy(box)\n",
    "            track_id = track.track_id\n",
    "            outputs.append(np.array([x1,y1,x2,y2,track_id], dtype=np.int))\n",
    "        if len(outputs) > 0:\n",
    "            outputs = np.stack(outputs,axis=0)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Convert bbox from xc_yc_w_h to xtl_ytl_w_h\n",
    "    Thanks JieChen91@github.com for reporting this bug!\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    # (cx,cy,w,h) -> (topleft_x, topleft_y, w, h) \n",
    "    def _xywh_to_tlwh(bbox_xywh):\n",
    "        if isinstance(bbox_xywh, np.ndarray):\n",
    "            bbox_tlwh = bbox_xywh.copy()\n",
    "        elif isinstance(bbox_xywh, torch.Tensor):\n",
    "            bbox_tlwh = bbox_xywh.clone()\n",
    "        bbox_tlwh[:,0] = bbox_xywh[:,0] - bbox_xywh[:,2]/2.\n",
    "        bbox_tlwh[:,1] = bbox_xywh[:,1] - bbox_xywh[:,3]/2.\n",
    "        return bbox_tlwh\n",
    "\n",
    "\n",
    "    def _xywh_to_xyxy(self, bbox_xywh):\n",
    "        x,y,w,h = bbox_xywh\n",
    "        x1 = max(int(x-w/2),0)\n",
    "        x2 = min(int(x+w/2),self.width-1)\n",
    "        y1 = max(int(y-h/2),0)\n",
    "        y2 = min(int(y+h/2),self.height-1)\n",
    "        return x1,y1,x2,y2\n",
    "\n",
    "    def _tlwh_to_xyxy(self, bbox_tlwh):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            Convert bbox from xtl_ytl_w_h to xc_yc_w_h\n",
    "        Thanks JieChen91@github.com for reporting this bug!\n",
    "        \"\"\"\n",
    "        x,y,w,h = bbox_tlwh\n",
    "        x1 = max(int(x),0)\n",
    "        x2 = min(int(x+w),self.width-1)\n",
    "        y1 = max(int(y),0)\n",
    "        y2 = min(int(y+h),self.height-1)\n",
    "        return x1,y1,x2,y2\n",
    "\n",
    "    def _xyxy_to_tlwh(self, bbox_xyxy):\n",
    "        x1,y1,x2,y2 = bbox_xyxy\n",
    "\n",
    "        t = x1\n",
    "        l = y1\n",
    "        w = int(x2-x1)\n",
    "        h = int(y2-y1)\n",
    "        return t,l,w,h\n",
    "    \n",
    "    def _get_features(self, bbox_xywh, ori_img):\n",
    "        im_crops = []\n",
    "        for box in bbox_xywh:\n",
    "            x1,y1,x2,y2 = self._xywh_to_xyxy(box)\n",
    "            im = ori_img[y1:y2,x1:x2] # original image의 크기에 맞게 crop\n",
    "            im_crops.append(im)\n",
    "        if im_crops:\n",
    "            features = self.extractor(im_crops) # crop한 이미지에서 특징 추출\n",
    "        else:\n",
    "            features = np.array([])\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
